import csv
import time
import sys
import os
from Utils.TSet import cal_size
from collections import defaultdict
from Conjunctive import EDB, PARAMS, search
from Disjunctive import search_union_no_overlap_encrypted
import logging, pickle


logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('boolean_query.log')
    ]
)


def get_temp_filename(prefix, suffix):
    timestamp = int(time.time())
    return f"{prefix}_{timestamp}{suffix}"


def parse_and_generate_initial_index(path):
    """è§£æå¸ƒå°”ç»“æ„æ–‡ä»¶ï¼Œç”Ÿæˆåˆå§‹ç´¢å¼•æ–‡ä»¶"""
    tag_to_docs = defaultdict(list)
    d_to_tags = defaultdict(list)
    with open(path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        for row in reader:
            if not row or len(row) < 3:
                continue
            D, tag, *docs = row
            d_to_tags[D.strip()].append(tag.strip())
            tag_to_docs[tag.strip()].extend([d.strip() for d in docs if d.strip()])

    
    INITIAL_WID = get_temp_filename("initial_wid", ".csv")
    INITIAL_IDW = get_temp_filename("initial_idw", ".csv")
    

    with open(INITIAL_WID, 'w', newline='', encoding='utf-8') as wf:
        writer = csv.writer(wf)
        for tag, docs in tag_to_docs.items():
            writer.writerow([tag] + list(set(docs)))


    doc_to_tags = defaultdict(list)
    for tag, docs in tag_to_docs.items():
        for doc in set(docs):
            doc_to_tags[doc].append(tag)
    with open(INITIAL_IDW, 'w', newline='', encoding='utf-8') as wf:
        writer = csv.writer(wf)
        for doc, tags in doc_to_tags.items():
            writer.writerow([doc] + tags)

    return d_to_tags, INITIAL_WID, INITIAL_IDW


def get_docids_for_D(d_to_tags, keys, edb, msk, D_list):
    """ä¸ºæ¯ä¸ªD_iæ‰§è¡Œå¹¶é›†æŸ¥è¯¢ï¼Œè·å–å¯¹åº”çš„æ–‡æ¡£IDé›†åˆ"""
    d_to_docids = {}
    for D in D_list:
        if D not in d_to_tags:
            logging.warning(f"âš ï¸ æŒ‡å®šçš„D_i '{D}' ä¸åœ¨è¾“å…¥æ–‡ä»¶ä¸­ï¼Œè·³è¿‡")
            continue
            
        tags = d_to_tags[D]
        logging.info(f"\nğŸ” å¹¶é›†æŸ¥è¯¢ D={D}, tags={tags}")
        t0 = time.time()
        res = search_union_no_overlap_encrypted(msk, tags, edb, keys)
        decoded = [r.decode() for r in res]
        t1 = time.time()
        logging.info(f"âœ… å¾—åˆ°æ–‡æ¡£ç¼–å·: {decoded}")
        logging.info(f"â° å¹¶é›†è€—æ—¶: {t1 - t0:.3f} ç§’")
        d_to_docids[D] = decoded
    return d_to_docids


def save_wid_idw_from_d_map(d_to_docs):
    """å°†D_iä¸æ–‡æ¡£IDçš„å¯¹åº”å…³ç³»ä¿å­˜ä¸ºæ–°çš„ç´¢å¼•æ–‡ä»¶"""
    # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
    FINAL_WID = get_temp_filename("final_wid", ".csv")
    FINAL_IDW = get_temp_filename("final_idw", ".csv")
    
    with open(FINAL_WID, 'w', newline='', encoding='utf-8') as wf:
        writer = csv.writer(wf)
        for D, docs in d_to_docs.items():
            writer.writerow([D] + docs)

    doc_to_ds = defaultdict(list)
    for D, docs in d_to_docs.items():
        for doc in docs:
            doc_to_ds[doc].append(D)

    with open(FINAL_IDW, 'w', newline='', encoding='utf-8') as wf:
        writer = csv.writer(wf)
        for doc, Ds in doc_to_ds.items():
            writer.writerow([doc] + Ds)
            
    return FINAL_WID, FINAL_IDW


def run_final_intersection(D_list, keys, edb, msk):
    """å¯¹æŒ‡å®šçš„D_iåˆ—è¡¨æ‰§è¡Œäº¤é›†æŸ¥è¯¢ï¼Œè¿”å›åŒæ—¶åŒ…å«è¿™äº›D_içš„æ–‡æ¡£ID"""
    if not D_list:
        logging.warning("âš ï¸ æ²¡æœ‰æŒ‡å®šD_iåˆ—è¡¨ï¼Œäº¤é›†æŸ¥è¯¢ä¸­æ­¢")
        return []
    logging.info(f"\nğŸš€ æ‰§è¡Œæœ€ç»ˆäº¤é›†æŸ¥è¯¢: {D_list}")
    t0 = time.time()
    result = search(msk, D_list, edb, keys)
    t1 = time.time()
    decoded = [x.decode() for x in result]
    logging.info(f"âœ… äº¤é›†æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {t1 - t0:.6f} ç§’")
    logging.info(f"ğŸ¯ æœ€ç»ˆäº¤é›†ç»“æœæ–‡æ¡£ç¼–å·ï¼š{decoded}")
    return decoded


if __name__ == "__main__":
    try:
   
        if len(sys.argv) < 4:
            print("ç”¨æ³•: python Boolean.py <query_structure_file> <inverted_index_file> <query_file>")
            print("ç¤ºä¾‹: python Boolean.py query_structure.csv inverted_index.csv query.txt")
            sys.exit(1)
        
  
        query_structure_file = sys.argv[1]
        inverted_index_file = sys.argv[2]
        query_file = sys.argv[3]
        
      
        if not os.path.exists(query_structure_file):
            raise FileNotFoundError(f"æŸ¥è¯¢ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨: {query_structure_file}")
        if not os.path.exists(inverted_index_file):
            raise FileNotFoundError(f"å€’æ’ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {inverted_index_file}")
        if not os.path.exists(query_file):
            raise FileNotFoundError(f"æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_file}")

       
        with open(query_file, 'r', encoding='utf-8') as f:
            d_list = [line.strip() for line in f if line.strip()]
        
        
        print("ğŸ“– è§£æå¸ƒå°”ç»“æ„æ–‡ä»¶å¹¶ç”Ÿæˆåˆå§‹ç´¢å¼•...")
        d_to_tags, INITIAL_WID, INITIAL_IDW = parse_and_generate_initial_index(query_structure_file)
        
       
        print("ğŸ“¦ åˆå§‹åŒ–åŠ å¯†æ•°æ®åº“...")
        keys = PARAMS()
        edb = EDB(5000000, 3)
        msk = edb.setup(INITIAL_WID, INITIAL_IDW, keys)
        
       
        print("\nğŸ“Š è®¡ç®—æŒ‡å®šD_içš„æ–‡æ¡£IDå¹¶é›†...")
        d_to_docids = get_docids_for_D(d_to_tags, keys, edb, msk, d_list)
        
      
        print("\nğŸ’¾ åŸºäºå¹¶é›†ç»“æœç”Ÿæˆæ–°çš„ç´¢å¼•æ–‡ä»¶...")
        FINAL_WID, FINAL_IDW = save_wid_idw_from_d_map(d_to_docids)
        
     
        print("ğŸ“¥ æ›´æ–°åŠ å¯†æ•°æ®åº“ä¸ºæ–°ç´¢å¼•...")
        msk = edb.setup(FINAL_WID, FINAL_IDW, keys)
        
      
        print(f"\nğŸŒŸ å¼€å§‹äº¤é›†æŸ¥è¯¢ï¼Œç›®æ ‡D_iåˆ—è¡¨: {d_list}")
        final_result = run_final_intersection(d_list, keys, edb, msk)
        
      
        print(f"\nğŸ‰ æœ€ç»ˆæŸ¥è¯¢ç»“æœ: {final_result}")
        
        tset_size_calc = cal_size(edb.tset)
        print(f"  - TSet é€»è¾‘é•¿åº¦è®¡ç®—: {tset_size_calc / 1024:.2f} KB")

        tset_size_dump = len(pickle.dumps(edb.tset))
        print(f"  - TSet åºåˆ—åŒ–å¤§å°  : {tset_size_dump / 1024:.2f} KB")

        xset_size_calc = len(edb.ct) * 32
        print(f"  - XSet é€»è¾‘é•¿åº¦è®¡ç®—: {xset_size_calc / 1024:.2f} KB")

        xset_size_dump = len(pickle.dumps(edb.ct))
        print(f"  - XSet åºåˆ—åŒ–å¤§å°  : {xset_size_dump / 1024:.2f} KB")
        
        
        for file in [INITIAL_WID, INITIAL_IDW, FINAL_WID, FINAL_IDW]:
            try:
                if os.path.exists(file):
                    os.remove(file)
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {file} å¤±è´¥: {e}")
        
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
    except KeyError as e:
        print(f"âŒ æŒ‡å®šçš„D_i {e} ä¸åœ¨è¾“å…¥æ–‡ä»¶ä¸­")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
